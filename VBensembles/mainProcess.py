import time
import voteBoosting as vb
from scipy.stats import beta
import metricsMethod
from sklearn.model_selection import KFold
import os
import itertools
import numpy as np
from sklearn.preprocessing import MinMaxScaler
# vote-boosting ensemble
# 基础算法就是随机森林
# 1. 开始，各个样本的权重都是一样的
# 2.之后根据得到的票数来更新各样本的权重

#计算时间，修饰器
def calcTime(file_name):
    def decorator(func):
        def wrapper(*args, **kwargs):
            start_time = time.clock()
            precise, recall = func(*args, **kwargs)
            end_time = time.clock()
            print('此次运行了时间：', end_time - start_time)
            with open(file_name + '/time.txt', 'a') as f:
                f.write('运行时间：' + str(end_time - start_time) + '\r\n')
            with open(file_name + '/result.txt', 'a') as f:
                f.write('精确率：' + str(precise) + ',' + '召回率：' + str(recall) + '\r\n')
            return precise, recall
        return wrapper
    return decorator



def algorithmPro(vote_boosting_ensemble,train,one_train_label,a_b):
    for classifier_index in range(vote_boosting_ensemble.en_size):
        train_result = vote_boosting_ensemble.trainEnsemble(train,one_train_label,classifier_index)
        vote_boosting_ensemble.t_x += np.array([result > 0 for result in train_result])
        la_x = (vote_boosting_ensemble.t_x + 1) / (classifier_index + 2)
##        print('la_t:',la_x)
        tmp_weight = np.array([float(beta.ppf(la,a_b,a_b)) for la in la_x])
##        print('tmp_weight:',tmp_weight)
##        print('sum(tmp_weight):',sum(tmp_weight))
        vote_boosting_ensemble.weights = tmp_weight / sum(tmp_weight)
        # print('vote_boosting_ensemble.weights:',vote_boosting_ensemble.weights)
    #print('权重更新完毕')

    # for data in test:
    #     one_result = 0
    #     for classifier in vote_boosting_ensemble.base_classifier:
    #         print('开始预测数据')
    #         result = classifier.predict(data)
    #         print('预测出结果')
    #         one_result += result/vote_boosting_ensemble.en_size
    #     if one_result > 0:
    #         all_result.append(1)
    #     else:
    #         all_result.append(-1)
    return vote_boosting_ensemble

    # all_result = np.zeros(len(test))
    # for classifier in vote_boosting_ensemble.base_classifier:
    #     all_result += classifier.predict(test)
    # all_result = all_result / vote_boosting_ensemble.en_size
    # for index,one_result in enumerate(all_result):
    #     if one_result > 0:
    #         all_result[index] = 1
    #     else:
    #         all_result[index] = -1
    # return all_result

# @calcTime(file_name = 'result1.0')
# def mainCode(test_true_index, train_file_name, test_file_name):
#     #提取数据的参数
#     side = 12
#     center = 6564
#     dis_threshold = 1
#     #集成算法中基础算法的个数
#     en_size = 501
#     #beta分布的参数,这是因为a,b的值是相同的
#     a_b_all = [0.25, 0.5, 0.75, 1.0, 1.25, 1.5, 2.5, 5, 10, 20, 40]
    
#     #提取出训练数据
#     train_data = metricsMethod.getData(train_file_name, side, center, dis_threshold)
#     min_max_scaler = MinMaxScaler()
#     train_data = min_max_scaler.fit_transform(train_data)

#     #-------------------------------------------
#     train_label = metricsMethod.getLabel(len(train_data))
#     #---------------------------------------------

#     #提取出测试数据
#     test_data = metricsMethod.getData(test_file_name, side, center, dis_threshold)
#     test_data = min_max_scaler.fit_transform(test_data)

#     ten_data = KFold(n_splits = 10)
#     max_acc = -float('inf')
#     max_a_b = -1
#     i = 0
#     #存储所有的指数
#     for a_b in a_b_all:
#         #数据进行10折交叉验证
#         # ten_data = KFold(len(train_data), n_splits = 10)
#         # ten_data = KFold(n_splits = 10)
#         # vote_boosting_ensemble = vb.VoteEnsemble(en_size,len(train))
#         one_acc_sum = 0
#         for train_index,test_index in ten_data.split(train_data):
#             i += 1
#             #print('第{0}次循环',format(i))
#             #得出数据
#             train = np.array([train_data[index] for index in train_index])
#             one_train_label = np.array([train_label[index] for index in train_index])
#             test = np.array([train_data[index] for index in test_index])
#             test_label = np.array([train_label[index] for index in test_index])

#             vote_boosting_ensemble = vb.VoteEnsemble(en_size,len(train))
#             #print('开始求结果')

#             all_result = algorithmPro(vote_boosting_ensemble,train,one_train_label,test,a_b)
#             #print('得出结果')
#             one_acc_sum += metricsMethod.getAccuracy(all_result,test_label)
#             #print('得出精确率:',one_acc_sum)
#         one_acc_sum /= 10
#         if one_acc_sum > max_acc:
#             max_acc = one_acc_sum
#             max_a_b = a_b
#     print('预测出来：',max_a_b)
#     vote_boosting_ensemble = vb.VoteEnsemble(en_size,len(train_data))
#     test_class = algorithmPro(vote_boosting_ensemble,train_data,train_label,test_data,max_a_b)
#     return metricsMethod.getTestAccAndRecall(test_class, test_true_index)


def mainCode(train_file_name,side,center,dis_threshold):
    #提取数据的参数
    # side = 12
    # center = 6564
    # dis_threshold = 1
    #集成算法中基础算法的个数
    en_size = 501
    #beta分布的参数,这是因为a,b的值是相同的
    a_b_all = [0.25, 0.5, 0.75, 1.0, 1.25, 1.5, 2.5, 5, 10, 20, 40]
    
    #提取出训练数据
    train_data = metricsMethod.getData(train_file_name, side, center, dis_threshold)
    min_max_scaler = MinMaxScaler()
    train_data = min_max_scaler.fit_transform(train_data)

    #-------------------------------------------
    train_label = metricsMethod.getLabel(len(train_data))
    #---------------------------------------------

    #提取出测试数据
    # test_data = metricsMethod.getData(test_file_name, side, center, dis_threshold)
    # test_data = min_max_scaler.fit_transform(test_data)

    ten_data = KFold(n_splits = 10)
    max_acc = -float('inf')
    max_a_b = -1
    i = 0
    #存储所有的指数
    for a_b in a_b_all:
        #数据进行10折交叉验证
        # ten_data = KFold(len(train_data), n_splits = 10)
        # ten_data = KFold(n_splits = 10)
        # vote_boosting_ensemble = vb.VoteEnsemble(en_size,len(train))
        one_acc_sum = 0
        for train_index,test_index in ten_data.split(train_data):
            i += 1
            print('第{0}次循环',format(i))
            #得出数据
            train = np.array([train_data[index] for index in train_index])
            one_train_label = np.array([train_label[index] for index in train_index])
            test = np.array([train_data[index] for index in test_index])
            test_label = np.array([train_label[index] for index in test_index])

            vote_boosting_ensemble = vb.VoteEnsemble(en_size,len(train))
            #print('开始求结果')
            
            # all_result = algorithmPro(vote_boosting_ensemble,train,one_train_label,test,a_b)

            vote_boosting_ensemble = algorithmPro(vote_boosting_ensemble,train,one_train_label,a_b)

            all_result = np.zeros(len(test))
            for classifier in vote_boosting_ensemble.base_classifier:
                all_result += classifier.predict(test)
            all_result = all_result / vote_boosting_ensemble.en_size
            for index,one_result in enumerate(all_result):
                if one_result > 0:
                    all_result[index] = 1
                else:
                    all_result[index] = -1
            #print('得出结果')
            one_acc_sum += metricsMethod.getAccuracy(all_result,test_label)
            #print('得出精确率:',one_acc_sum)
        one_acc_sum /= 10
        if one_acc_sum > max_acc:
            max_acc = one_acc_sum
            max_a_b = a_b
    print('预测出来：',max_a_b)
    vote_boosting_ensemble = vb.VoteEnsemble(en_size,len(train_data))
    return algorithmPro(vote_boosting_ensemble,train_data,train_label,max_a_b)
    # test_class = algorithmPro(vote_boosting_ensemble,train_data,train_label,test_data,max_a_b)
    # return metricsMethod.getTestAccAndRecall(test_class, test_true_index)


if __name__ == '__main__':
    file_name = 'H:/数据/comparedExperiment/testData'
    train_file_name = 'H:/数据/comparedExperiment/trainData/trainDatas'
    result_file_name = 'result1.0'
    test_true_index = [
        [34, 24, 301, 105, 535, 599, 83],
        [280, 1401, 70, 9, 8, 2682, 882, 241, 2245, 772, 2399, 694, 115, 134, 1842, 163, 489, 4, 402, 197, 2200, 217, 2162, 2170, 604, 824, 2085, 914, 800, 109, 2194, 114, 488, 2203, 136, 2377, 117, 229, 2178, 2366, 3, 474, 1268, 2797, 101, 97, 936, 2708, 2191, 2166, 450, 1770, 1811, 2417, 906, 928, 2064, 1800, 95, 2328, 1567, 2192, 778, 2111, 2063, 1771, 1502, 2411, 102, 2330, 2033, 1392, 2935, 2483, 372, 1869, 1479, 1808, 1809, 2051, 1784, 281, 1798, 1477, 510, 2046, 2338, 1790, 88, 85, 2171, 1707, 52, 2457, 64, 105, 55, 1807, 2224, 2028, 1395, 2058, 1853, 1500, 1495, 76, 33, 1502, 2064, 2399, 327, 58, 2812, 1499, 176, 1714, 137, 361, 2674, 3015, 1291, 194, 347, 2418, 1068, 485, 1065, 156, 2973, 264, 2075, 2155, 602, 263, 1561, 2806, 2631, 2326, 752, 220, 816, 2260, 237, 139, 123, 2871, 1993, 2173, 50, 1066, 289, 967, 801, 1992, 3045, 2937, 1701, 1078, 169, 1092, 162, 2865, 2700, 1595, 2067, 607, 3052, 149, 1102, 983, 1741, 1011, 3035, 2849, 888, 1937, 723, 1333, 261, 626, 221, 1085, 1710, 2333, 2688, 39, 1308, 215, 788, 257, 338, 2828, 326, 1935, 2665, 553, 729, 2168, 817, 590, 191, 1608, 249, 1709, 298, 599, 439, 651, 1590, 1720, 2597, 1861, 2780, 1756, 2782, 103, 1706, 2571, 84, 779, 2021, 1184, 2519, 2933, 660, 1942, 1642, 1694, 1911, 2056, 1978, 2711, 259, 621, 1754, 2721, 1577, 1762, 1482, 1763, 2044, 2398, 1645, 1736, 1947, 2453, 63, 2667, 2591, 3554, 1753, 1858, 1704, 3492, 1717, 2163, 2022, 1740, 801, 3262],
        [357, 7743, 1325, 269, 7644, 979, 5913, 5583, 4783, 5113, 414, 1098, 1101, 1076, 644, 6582, 5444, 618, 1234, 94, 1094, 290, 4845, 7700, 22, 280, 4982, 1072, 7321, 385, 110, 328, 6262, 332, 121, 5315, 3883, 2812, 4262, 5088, 1073, 6629, 508, 23, 401, 523, 4932, 5852, 1977, 4691, 281, 4349, 657, 6209, 2841, 403, 7310, 342, 4563, 479, 5144, 3855, 637, 45, 5156, 2843, 87, 547, 406, 4538, 5022, 7199, 7173, 7349, 251, 4289, 6261, 478, 2982, 6257, 412, 6559, 3146, 112, 107, 6301, 5323, 4720, 179, 18, 2261, 469, 485, 291, 4392, 4913, 4968, 421, 181, 273, 568, 7453, 105, 5082, 417, 418, 4899, 4786, 5787, 962, 397, 550, 8, 2776, 512, 4689, 19, 472, 4650, 564, 464, 321, 557, 890, 4690, 461, 319, 5055, 106, 402, 407, 255, 317, 325, 6905, 415, 192, 303, 134, 444, 8014, 98, 496, 7767, 490, 599, 352, 7186, 6316, 7053, 5980, 6157, 6288, 7659, 6144, 7141, 5909, 7964, 7751, 8185, 7480, 6746, 804, 7850, 6770, 6317, 6530, 6078, 4284, 6003, 7709, 8183, 6063, 4924, 7668, 8037, 6954, 1086, 7339, 237, 6797, 7736, 6531, 7554, 7354, 7744, 2926, 6454, 7165, 1066, 5906, 7990, 36, 6010, 6093, 7398, 6850, 7187, 6781, 7733, 7698, 6726, 2729, 8026, 7278, 6633, 7712, 7481, 7729, 6564, 7482, 7936, 7197, 957, 3342, 8047, 4923, 7064, 7040, 8015, 6477, 7280, 7971, 6603, 7049, 7327, 8141, 7740, 8169, 8132, 7486, 6634, 4926, 7723, 4866, 4240, 6225, 6821, 6672, 7382, 6849, 7070, 7589, 6556, 7297, 5087, 5106, 231, 4281, 7525, 6005, 8079, 8137, 4477, 7582, 7434, 2891, 4181, 7388, 7667, 6436, 7236, 6844, 5868, 7047, 7465, 168, 7312, 6950, 7471, 6683, 7139, 5884, 7124, 7814, 7816, 6147, 7968, 6111, 6677, 7478, 6909, 6079, 7954, 7088, 7946, 4878, 5956, 7247, 5957, 7494, 7356, 5929, 215, 7807, 6406, 7730, 7156, 6489, 6899, 6242, 7134, 6882, 6823, 6773, 6776, 6412, 7902, 6832, 7316, 5973, 7541, 7227, 5879, 7395, 6415, 6685, 5908, 5146, 6826, 6013, 5903, 7353, 7799, 6108, 5882, 7403, 6684, 7180, 5865, 3875, 7366, 2687, 7616, 6977, 6112, 6434, 8033, 6428, 5995, 3276, 7242, 6290, 4847, 4921, 6208, 7185, 8142, 6703, 7949, 7201, 6458, 4696, 7422, 4804, 7115, 6056, 554, 3790, 7953, 8175, 233, 6665, 5846, 8193, 7080, 8060, 7590, 4537, 7951, 7371, 6266, 7091, 6808, 6765, 6028, 7943, 7140, 6040, 6941, 6156, 7625, 7283, 6697, 753, 6281, 4549, 6963, 4379, 6969, 7795, 6632, 7923, 6132, 6153, 6018, 6753, 8140, 6987, 7123, 7369, 7315, 721, 7133, 6670, 4928, 4856, 6700, 7580, 6878, 3367, 5881, 7456, 6331, 7286, 6980, 6927, 4711, 7944, 7490, 6673, 7862, 5016, 4803, 4306, 6241, 7467, 7078, 6992, 6917, 4489, 5848, 8008, 6293, 6893, 7796, 6270, 6715, 7870, 5125, 6555, 6856, 7146, 4816, 4236, 7089, 6190, 6085, 7172, 2276, 5159, 7567, 4933, 7942, 4352, 6524, 6366, 7918, 6355, 5976, 3323, 4594, 7776, 7179, 6988, 7121, 7183, 7881, 6727, 6705, 4792, 8174, 4433, 7798, 6960, 6041, 6964, 7821, 6082, 6891, 7129, 6365, 6278, 753, 1086, 5980, 6078, 6157, 6672, 6746, 7709, 7964, 8185, 3148, 4030, 3575, 8131, 4034, 3192, 3808, 6819, 2679, 6817, 3974, 2290, 1332, 1520, 5556],
        [2545, 11111, 5633, 11555, 5812, 3246, 152, 5783, 12231, 4403, 1045, 3425, 1752, 10926, 1700, 7141, 11475, 5944, 6219, 3980, 11838, 3774, 12104, 12442, 6639, 4642, 8019, 8070, 8400, 9556, 8986, 7768, 4612, 8191, 12577, 1311, 7946, 10257, 4458, 6431, 4597, 6047, 5052, 5171, 9608, 8848, 7925, 5014, 9522, 4781, 3855, 220, 3504, 5691, 1325, 3283, 4614, 5435, 2224, 3515, 2219, 4490, 10903, 1716, 8929, 11735, 10221, 2262, 179, 10235, 7137, 10217, 210, 2260, 10548, 10843, 209, 12309, 12237, 12182, 11020, 10915, 199, 1345, 5693, 2277, 4023, 5718, 2236, 12339, 11525, 2391, 1392, 11530, 2249, 5699, 661, 10942, 267, 1319, 11418, 10948, 11543, 12294, 2215, 1692, 11019, 1737, 11544, 5429, 1047, 2237, 2464, 1044, 2235, 1953, 10911, 5634, 2851, 3044, 188, 2285, 2216, 2372, 2398, 2737, 2400, 5758, 185, 1832, 240, 244, 222, 277, 2297, 282, 105, 237, 2467, 2097, 2567, 2292, 5628, 7133, 8966, 5710, 230, 2243, 2369, 5631, 221, 11007, 10920, 11501, 7517, 2492, 7142, 303, 2516, 269, 2295, 2190, 5717, 101, 1207, 2307, 224, 1442, 1036, 10853, 7272, 246, 2299, 1645, 11636, 242, 239, 7273, 5754, 11539, 2225, 2314, 1686, 1029, 3194, 1785, 299, 1820, 1842, 11001, 90, 317, 1040, 292, 1976, 53, 1035, 2418, 1492, 10779, 372, 1912, 1034, 1677, 1690, 11089, 536, 7156, 1981, 7278, 2350, 2513, 3049, 9070, 3187, 1684, 10776, 83, 88, 1679, 92, 11208, 2066, 7105, 1844, 2351, 9123, 1807, 7256, 61, 2404, 1809, 375, 1441, 1917, 3022, 1828, 298, 271, 2457, 309, 290, 294, 7315, 7198, 5, 10786, 76, 54, 11132, 6, 7312, 1283, 1670, 10787, 1, 3074, 11456, 163, 131, 9116, 2602, 1850, 11017, 5621, 1429, 1798, 7204, 1790, 7196, 270, 1813, 11134, 2056, 2061, 1874, 5809, 1971, 834, 11090, 11144, 140, 6995, 10773, 1985, 7294, 1929, 311, 6530, 2034, 1056, 11135, 9086, 1030, 5781, 1603, 6556, 11540, 1875, 5753, 468, 2069, 21, 5535, 1950, 1863, 1771, 10701, 1928, 1848, 5575, 150, 1406, 1896, 1780, 10774, 1859, 1072, 7216, 143, 1805, 10814, 102, 1858, 6971, 10793, 7236, 10775, 347, 2385, 304, 10722, 5831, 3008, 1876, 2546, 5772, 1878, 10794, 5875, 3129, 11166, 1927, 11012, 1151, 109, 2028, 7068, 2560, 7226, 1922, 1892, 1660, 1883, 1656, 1867, 11136, 11163, 10792, 1856, 3174, 7241, 5624, 11117, 1795, 12, 160, 753, 1885, 2540, 1667, 2325, 5625, 1861, 6963, 32, 1865, 2551, 2177, 7062, 11104, 2038, 1880, 11534, 9157, 1666, 18, 6983, 52, 1904, 2379, 2014, 10150, 7001, 1864, 5491, 11156, 1109, 15, 1778, 11951, 5956, 30, 1871, 7302, 9045, 11179, 2344, 1906, 1926, 110, 5745, 11173, 7070, 2535, 2554, 11659, 2166, 11184, 2407, 1815, 320, 5902, 1935, 2559, 11176, 11168, 1901, 1884, 2550, 153, 2025, 113, 7054, 1870, 1890, 154, 5857, 11660, 1903, 11101, 5862, 5871, 11183, 1654, 10800, 1900, 1947, 345, 5, 101, 143, 152, 237, 1034, 1040, 1109, 1429, 1492, 1771, 1780, 1820, 1901, 2567, 3425, 5535, 5699, 7156, 10911, 12231, 11502, 474, 10931, 11256, 5774, 12820, 11070, 326, 8767, 5746, 1643, 1349, 3188, 1535, 11274, 8408, 1010, 2834, 10528, 12662, 3217, 11428, 9723, 9789, 10209, 6811, 3647, 2835, 5798, 7811, 3698, 3824, 9667, 9725, 10392, 5703, 5724, 2263, 3491, 5349, 1219, 7769, 4752, 1707, 7706, 682, 3857, 5202, 10817, 1208, 697, 4596, 10472, 2878, 5690, 5705, 11896, 1221, 1335, 5221, 691, 681, 8839, 10848, 1194, 1764, 11358, 5801, 512, 5331, 2278, 1213, 2488, 308, 6700, 664, 11258, 824, 6675, 1193, 4966, 1566, 2877, 1377, 4895, 1197, 1063, 12215, 3626, 1218, 2272, 3314, 1477, 1524, 11695, 5806, 516, 1512, 5332, 10868, 380, 1443, 10782, 1450, 11076, 1769, 1031, 1225, 10932, 1341, 2064, 918, 10845, 104, 2593, 5751, 5755, 2873, 8701, 2259, 2068, 2609, 8498, 1201, 2490, 10935, 2293, 2906, 1366, 1165, 1521, 306, 10835, 11386, 2241, 2116, 1456, 703, 2841, 5803, 692, 1061, 5579, 1459, 7181, 860, 11635, 5667, 5688, 97, 1765, 1037, 2596, 1742, 5821, 5702, 5549, 3017, 1538, 1167, 1350, 678, 1379, 1186, 11406, 1376, 9093, 2390, 2081, 1557, 11515, 582, 2993, 1779, 10854, 2125, 11604, 11251, 1763, 11193, 1074, 1146, 2989, 10923, 1203, 5707, 10588, 11436, 2226, 896, 2436, 8485, 1370, 497, 731, 11261, 1768, 10882, 650, 1783, 11242, 894, 2244, 11394, 11003, 2915, 1198, 1484, 2280, 1375, 857, 1174, 1800, 1125, 1228, 12022, 11038, 3196, 968, 3056, 1930, 2615, 1650, 2987, 1211, 10907, 788, 848, 1452, 5627, 1559, 519, 11010, 4870, 12191, 564, 514, 10950, 1806, 1818, 1460, 10079, 5784, 1525, 3013, 5738, 3116, 3111, 1553, 1022, 11440, 12018, 1497, 1243, 852, 1500, 1407, 570, 1644, 1388, 1537, 1481, 310, 3186, 1163, 1343, 5824, 11401, 1381, 11018, 2147, 479, 1506, 2233, 567, 2933, 1499, 3047, 10866, 11459, 2845, 628, 5789, 1836, 3068, 1095, 5828, 2100, 1227, 2977, 1286, 1210, 1378, 1586, 119, 448, 11278, 2571, 1307, 11572, 355, 2397, 11186, 1490, 742, 1202, 1131, 1179, 1062, 12838, 2304, 2766, 313, 1191, 3026, 891, 249, 812, 1196, 3141, 6490, 11565, 3096, 2579, 11541, 1396, 1448, 1609, 5932, 611, 3030, 11119, 2052, 139, 11644, 1083, 10108, 1432, 9003, 1123, 2921, 1767, 6477, 2920, 11641, 1602, 2570, 11681, 1473, 1766, 809, 10863, 649, 1642, 1905, 1550, 7232, 2300, 1444, 1894, 11289, 2822, 11039, 1009, 11469, 1242, 1565, 77, 813, 1489, 1416, 3195, 1464, 1570, 1050, 5539, 11366, 10808, 2320, 5653, 11617, 11286, 1996, 1139, 5880, 11006, 539, 972, 1461, 11027, 1152, 11110, 2326, 758, 965, 7382, 2401, 1462, 3050, 11648, 2762, 2129, 2173, 331, 1237, 1138, 2504, 1436, 11029, 1272, 5807, 1510, 3164, 2652, 1425, 3152, 1252, 11103, 10983, 1055, 11279, 10992, 1269, 2526, 9096, 1514, 1415, 1285, 10804, 2936, 342, 3169, 1994, 1068, 2917, 1888, 995, 346, 867, 325, 1882, 1623, 1420, 2555, 1282, 1116, 2960, 11642, 4913, 1108, 1239, 3006, 759, 12722, 2558, 1620, 11098, 1584, 1627, 2542, 1857, 1051, 1617, 11112, 11661, 115, 3160, 2131, 11487, 1472, 2007, 11656, 1634, 985, 10764, 10858, 1630, 1886, 1619, 5865, 414, 5610, 1231, 112, 1418, 1238, 3171, 2942, 334, 516, 852, 891, 1123, 1211, 1225, 1227, 1444, 1456, 1767, 1800, 4913, 5539, 11695, 11502, 7556, 4669, 4923, 4194, 12176, 11784, 12130, 12689, 430, 3757, 3094, 7470, 12502, 12192, 7718, 2643, 517, 6420, 4914, 9958, 5066, 8980, 4943, 11968, 4619, 4880, 9615, 8284, 6821, 4712, 8798, 8082, 9824, 8339, 4368, 5964],
        [10062, 14013, 7390, 16094, 14921, 11544, 7371, 11096, 10510, 7398, 2902, 3820, 14951, 14440, 14328, 6834, 10258, 7488, 304, 2292, 439, 11286, 11959, 14877, 15913, 10893, 10423, 10427, 3221, 14779, 7912, 2703, 11441, 11374, 13864, 14535, 18876, 11339, 17869, 365, 10306, 10768, 10801, 8064, 13741, 15412, 5, 11734, 19065, 14336, 2305, 18182, 8793, 646, 14508, 7432, 14900, 11881, 5992, 14746, 7612, 17151, 6874, 9834, 16470, 5851, 4188, 12517, 192, 16512, 17229, 14984, 8859, 14079, 10766, 17921, 16046, 14906, 134, 15427, 16310, 10804, 11097, 15208, 19273, 15136, 17968, 13775, 15547, 9686, 15893, 14728, 19316, 11589, 8945, 699, 1769, 14624, 4516, 17227, 14835, 16186, 8367, 2868, 12811, 16949, 14023, 14364, 12452, 15555, 14897, 14726, 14494, 1620, 10775, 1439, 15650, 1488, 1668, 7587, 16497, 332, 6808, 5806, 10600, 19279, 13190, 16252, 7090, 14805, 15056, 17153, 16517, 14774, 22, 562, 8495, 4248, 15569, 15182, 19396, 14311, 16299, 4459, 14741, 15801, 14612, 4604, 15234, 19209, 3455, 15442, 15162, 14980, 11209, 14690, 9, 14183, 15396, 7545, 16322, 14667, 15875, 5632, 16716, 14771, 15281, 14503, 10905, 5273, 12, 4911, 15790, 15154, 14776, 1263, 633, 15537, 4684, 14804, 15605, 14251, 13865, 15661, 43, 13515, 15086, 4513, 16828, 14632, 7464, 15824, 2122, 7468, 15892, 15065, 18081, 14673, 15120, 15764, 5907, 14630, 13822, 13401, 14966, 11300, 3103, 17338, 15622, 1760, 15057, 14340, 14523, 14367, 15189, 14653, 10746, 15228, 14650, 15413, 13916, 13877, 15644, 15209, 2070, 15156, 14250, 16291, 15871, 15532, 5672, 15497, 15054, 1437, 13452, 16051, 13505, 15927, 14705, 16020, 18144, 15448, 15775, 1445, 4228, 6391, 15490, 1777, 15445, 16290, 15803, 12356, 6040, 14789, 2231, 17797, 16333, 14722, 15713, 15051, 15988, 15720, 3666, 14398, 13801, 5585, 14459, 14509, 15814, 15703, 2932, 2774, 14729, 13411, 15678, 15821, 15945, 2456, 15139, 13613, 15910, 14434, 15981, 2613, 15944, 15749, 3126, 13948, 15796, 3970, 16194, 18014, 15938, 2178, 16152, 15962, 17955, 6187, 15204, 15915, 15783, 16033, 13620, 15722, 15933, 15748, 1807, 14615, 6193, 13581, 13895, 16278, 14490, 14217, 16764, 4310, 6070, 15529, 2428, 14447, 2242, 15483, 14752, 15943, 15924, 1736, 13976, 15482, 16107, 14346, 1524, 14377, 15224, 1747, 1585, 1786, 15474, 14466, 5973, 2812, 4458, 15746, 5438, 3082, 304, 3820, 4310, 7468, 13505, 14336, 14440, 14508, 14673, 14951, 16046, 13533, 13930, 10471, 10468, 10640, 10913, 318, 13742, 11068, 13909, 14185, 13571, 7325, 9944, 10671, 10246, 14797, 909, 10943, 11482, 15363, 10160, 10474, 10216, 10922, 12714, 14720, 913, 9945, 10122, 10647, 10549, 10289, 10542, 11384, 10093, 10935, 6864, 7015, 11192, 11512, 1038, 10035, 10235, 408, 13868, 370, 11088, 13034, 10684, 10662, 659, 7241, 12254, 14724, 10264, 10234, 10033, 10240, 9974, 11322, 10286, 14601, 10928, 193, 10634, 16858, 10917, 13882, 10633, 10836, 11111, 10965, 10475, 10270, 13521, 10931, 7819, 14878, 11503, 10918, 11105, 202, 10641, 10473, 10287, 14428, 10425, 14102, 10320, 11351, 11580, 10608, 6971, 10446, 11037, 10927, 579, 10055, 6914, 11557, 11138, 10955, 9835, 11599, 11085, 10802, 10919, 10653, 901, 14292, 491, 10522, 11468, 14356, 7067, 10285, 9975, 14707, 9947, 14299, 10926, 11581, 14040, 13691, 11383, 11575, 10281, 11146, 13812, 11541, 10692, 14644, 11579, 11444, 9852, 14047, 13712, 10526, 10921, 10041, 11566, 13817, 18897, 10670, 12176, 10821, 9892, 13749, 11373, 13528, 598, 7088, 5717, 13418, 10309, 6824, 7073, 11379, 11047, 10226, 504, 10525, 306, 10578, 19259, 13769, 14176, 11476, 10469, 9961, 311, 14549, 11315, 14026, 11283, 14302, 13476, 10531, 14275, 1490, 11161, 13835, 6326, 11310, 14180, 3462, 11104, 14727, 170, 10834, 590, 14085, 433, 12308, 11558, 11596, 13704, 209, 11189, 13560, 13651, 6355, 14200, 7075, 15408, 519, 10290, 13770, 9919, 631, 11500, 10916, 13816, 14332, 10904, 11389, 11048, 508, 1001, 10997, 15613, 12085, 14041, 14017, 10763, 327, 14301, 14930, 7382, 17315, 3742, 44, 5862, 14214, 16082, 14168, 3774, 13929, 558, 4107, 10576, 2616, 13728, 11290, 14899, 14477, 15374, 13999, 6618, 4181, 13412, 10233, 13658, 15510, 13618, 14169, 14082, 14556, 18075, 701, 19349, 17556, 7401, 14072, 2647, 16229, 14431, 13696, 13813, 7946, 13436, 7290, 4940, 13636, 13853, 1354, 14184, 10256, 13848, 14570, 2139, 15024, 13954, 1932, 14651, 13890, 9934, 11513, 13804, 14614, 13973, 13844, 14480, 15257, 14170, 15761, 14402, 13837, 3748, 3469, 16216, 13407, 7430, 14160, 3361, 14123, 15211, 14672, 8889, 1818, 2162, 13884, 4254, 1920, 4790, 8211, 4376, 14055, 2007, 4440, 3744, 16141, 14948, 2054, 3930, 3013, 14166, 13949, 16200, 5598, 3344, 15682, 13932, 3119, 4801, 9007, 15674, 5696, 17531, 8489, 5255, 15982, 5930, 5684, 6080, 5561, 15662, 2230, 15664, 15602, 15921, 5665, 3538, 3387, 3048, 2580, 1856, 4314, 5137, 5684, 7015, 9007, 9945, 10471, 10671, 10836, 10913, 10916, 11351, 13560, 14047, 19349, 7850, 5506, 11908, 16860, 879, 17432, 16425, 9654, 18786, 10876, 17372, 6549, 11433, 12595, 8184, 13236, 7006, 10776, 12159, 3604, 18985, 2874, 11986, 8116, 18972, 8184, 11986]
    ]
    side = 12
    center = 6564
    dis_threshold = 1
    start_time = time.clock()
    print('开始训练模型')
    vote_boosting_ensemble = mainCode(train_file_name,side,center,dis_threshold)
    print('训练结束')
    end_time = time.clock()
    train_time = end_time - start_time
    
    list_dir = os.listdir(file_name)

    #这里的指标分别是精确率，召回率，运行时间
    all_metrics = []
    min_max_scaler = MinMaxScaler()
    for index,dir_name in enumerate(list_dir):
        print('dir:',dir_name)
        start_time = time.clock()
        test_data = metricsMethod.getData(file_name + '/' + dir_name, side, center, dis_threshold)
        test_data = min_max_scaler.fit_transform(test_data)
        all_result = np.zeros(len(test))
        for classifier in vote_boosting_ensemble.base_classifier:
            all_result += classifier.predict(test)
        all_result = all_result / vote_boosting_ensemble.en_size
        for index,one_result in enumerate(all_result):
            if one_result > 0:
                all_result[index] = 1
            else:
                all_result[index] = -1
        precise, recall = metricsMethod.getTestAccAndRecall(all_result, test_true_index[index])
        end_time = time.clock()
        all_time = end_time - start_time + train_time
        # @calcTime(result_file_name + '/' + dir_name)
        # precise, recall = mainCode(test_true_index[index], 
        #                            train_file_name,
        #                            file_name + '/' + dir_name)
        
        all_metrics.append([precise, recall])
        print('dir:{0},precise:{1},recall:{2},time:{3}'.format(dir_name,precise,recall,all_time))
        result = 'precise:' + str(precise) + ',recall：' + str(recall) + ',time:' + str(all_time)
        with open(result_file_name + '/' + dir_name + 'Result.txt','w') as f:
            f.write(result)
        # with open(result_file_name + '','w') as f:
    print(all_metrics)
